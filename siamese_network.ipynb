{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Triplet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/casia_cropped/000\n",
      "dataset/casia_cropped/001\n",
      "dataset/casia_cropped/002\n"
     ]
    }
   ],
   "source": [
    "img_paths = list()\n",
    "for i in range(3):\n",
    "    subject_id = str(i).zfill(3)\n",
    "    subject_path = f\"dataset/casia_cropped/{subject_id}\"\n",
    "    print(subject_path)\n",
    "    for j in range(5):\n",
    "        img_path = f\"{subject_path}/{subject_id}_{j}.jpg\"\n",
    "        img_paths.append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/casia_cropped/000/000_0.jpg',\n",
       " 'dataset/casia_cropped/000/000_1.jpg',\n",
       " 'dataset/casia_cropped/000/000_2.jpg',\n",
       " 'dataset/casia_cropped/000/000_3.jpg',\n",
       " 'dataset/casia_cropped/000/000_4.jpg',\n",
       " 'dataset/casia_cropped/001/001_0.jpg',\n",
       " 'dataset/casia_cropped/001/001_1.jpg',\n",
       " 'dataset/casia_cropped/001/001_2.jpg',\n",
       " 'dataset/casia_cropped/001/001_3.jpg',\n",
       " 'dataset/casia_cropped/001/001_4.jpg',\n",
       " 'dataset/casia_cropped/002/002_0.jpg',\n",
       " 'dataset/casia_cropped/002/002_1.jpg',\n",
       " 'dataset/casia_cropped/002/002_2.jpg',\n",
       " 'dataset/casia_cropped/002/002_3.jpg',\n",
       " 'dataset/casia_cropped/002/002_4.jpg']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths[0].split(\"/\")[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_paths = list()\n",
    "positive_paths = list()\n",
    "negative_paths = list()\n",
    "\n",
    "for anchor_path, positive_path, negative_path in permutations(img_paths, 3):\n",
    "    anchor_subject = anchor_path.split(\"/\")[-2]\n",
    "    positive_subject = positive_path.split(\"/\")[-2]\n",
    "    negative_subject = negative_path.split(\"/\")[-2]\n",
    "    \n",
    "    if (anchor_subject == positive_subject) and (anchor_subject != negative_subject):\n",
    "        anchor_paths.append(str(anchor_path))\n",
    "        positive_paths.append(str(positive_path))\n",
    "        negative_paths.append(str(negative_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(filename, target_shape):\n",
    "    \"\"\"\n",
    "    Load the specified file as a JPEG image, preprocess it and\n",
    "    resize it to the target shape.\n",
    "    \"\"\"\n",
    "\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, target_shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dict()\n",
    "target_shape = (160, 160)\n",
    "\n",
    "triplets = tf.zeros([0, 3, 160, 160, 3])\n",
    "# positive_imgs = list()\n",
    "# negative_imgs = list()\n",
    "\n",
    "for anchor_path, positive_path, negative_path in zip(anchor_paths, positive_paths, negative_paths):\n",
    "    try:\n",
    "        anchor_img = images[anchor_path]\n",
    "    except:\n",
    "        anchor_img = preprocess_image(anchor_path, target_shape)\n",
    "        images[anchor_path] = anchor_img\n",
    "    \n",
    "    try:\n",
    "        positive_img = images[positive_path]\n",
    "    except:\n",
    "        positive_img = preprocess_image(positive_path, target_shape)\n",
    "        images[positive_path] = positive_img\n",
    "        \n",
    "    try:\n",
    "        negative_img = images[negative_path]\n",
    "    except:\n",
    "        negative_img = preprocess_image(negative_path, target_shape)\n",
    "        images[negative_path] = negative_img\n",
    "    triplet = tf.stack([anchor_img, positive_img, negative_img], axis=0)\n",
    "    triplet = tf.expand_dims(triplet, axis=0)\n",
    "    triplets = tf.concat([triplets, triplet], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([600, 3, 160, 160, 3])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = tf.data.Dataset.from_tensors(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid `datasets`. `datasets` is expected to be a (nested) structure of `tf.data.Dataset` objects but encountered object of type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(tf\u001b[39m.\u001b[39mones((\u001b[39mlen\u001b[39m(triplets),)))\n\u001b[1;32m----> 2\u001b[0m triplet_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mzip((triplets, labels))\n",
      "File \u001b[1;32mc:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1264\u001b[0m, in \u001b[0;36mDatasetV2.zip\u001b[1;34m(datasets, name)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m   1219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mzip\u001b[39m(datasets, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1220\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a `Dataset` by zipping together the given datasets.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \n\u001b[0;32m   1222\u001b[0m \u001b[39m  This method has similar semantics to the built-in `zip()` function\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m ZipDataset(datasets, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4846\u001b[0m, in \u001b[0;36mZipDataset.__init__\u001b[1;34m(self, datasets, name)\u001b[0m\n\u001b[0;32m   4841\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid `datasets`. `datasets` is expected to be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4842\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39m(nested) structure of `tf.data.Dataset` objects. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4843\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPython `list` is not supported and you should use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4844\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39m`tuple` instead.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4845\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4846\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid `datasets`. `datasets` is expected to be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4847\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(nested) structure of `tf.data.Dataset` objects \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4848\u001b[0m                       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut encountered object of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(ds)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4849\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datasets \u001b[39m=\u001b[39m datasets\n\u001b[0;32m   4850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datasets,\n\u001b[0;32m   4852\u001b[0m     [ds\u001b[39m.\u001b[39melement_spec \u001b[39mfor\u001b[39;00m ds \u001b[39min\u001b[39;00m nest\u001b[39m.\u001b[39mflatten(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datasets)])\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid `datasets`. `datasets` is expected to be a (nested) structure of `tf.data.Dataset` objects but encountered object of type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "labels = tf.data.Dataset.from_tensor_slices(tf.ones((len(triplets),)))\n",
    "triplet_dataset = tf.data.Dataset.zip((triplets, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(triplet_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "x: 3\n"
     ]
    }
   ],
   "source": [
    "print(len(sample))\n",
    "print(\"x:\", len(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(triplets)\n",
    "\n",
    "triplet_dataset = triplet_dataset.shuffle(buffer_size=1024)\n",
    "train_dataset = triplet_dataset.take(round(num_samples * 0.8))\n",
    "val_dataset = triplet_dataset.skip(round(num_samples * 0.8))\n",
    "\n",
    "# train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "# train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "# val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "# val_dataset = val_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 160, 3)\n",
      "(160, 160, 3)\n",
      "(160, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(batch[0][0].shape)\n",
    "print(batch[0][1].shape)\n",
    "print(batch[0][2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Layer, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet.facenet import load_model as load_facenet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    y_true = tf.linalg.l2_normalize(x, axis=-1)\n",
    "    y_pred = tf.linalg.l2_normalize(y, axis=-1)\n",
    "    cosine_similarity = tf.reduce_sum(y_true * y_pred, axis=-1) # range [-1,1]\n",
    "    distance = 1 - cosine_similarity # range [0, 2]\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 0. 1. 0.]], shape=(2, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(2, 4), dtype=float32)\n",
      "tf.Tensor([0.        0.2928933], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test cosine_distance function\n",
    "\n",
    "arr0 = [\n",
    "    [1,1,1,1],\n",
    "    [1,0,1,0],\n",
    "]\n",
    "arr0 = tf.convert_to_tensor(arr0, dtype=tf.float32)\n",
    "arr1 = [\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "]\n",
    "arr1 = tf.convert_to_tensor(arr1, dtype=tf.float32)\n",
    "\n",
    "print(arr0)\n",
    "print(arr1)\n",
    "print(cosine_distance(arr0, arr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(templates, margin=0.4):\n",
    "    \n",
    "    anchor, positive, negative = templates\n",
    "    \n",
    "    positive_distance = cosine_distance(anchor,positive)\n",
    "    negative_distance = cosine_distance(anchor,negative)\n",
    "    \n",
    "    basic_loss = positive_distance - negative_distance + margin\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopting the TensorFlow Functional API\n",
    "\n",
    "def build_siamese_model(embedding_model):\n",
    "    target_shape = embedding_model.input_shape[1:3] + (3,)\n",
    "    anchor = Input(shape=target_shape, name='anchor_input')\n",
    "    A = embedding_model(anchor)\n",
    "\n",
    "    positive = Input(shape=target_shape, name='positive_input')\n",
    "    P = embedding_model(positive)\n",
    "\n",
    "    negative = Input(shape=target_shape, name='negative_input')\n",
    "    N = embedding_model(negative)\n",
    "\n",
    "    loss = Lambda(triplet_loss)([A, P, N])\n",
    "\n",
    "    model = Model(inputs=[anchor,positive,negative],outputs=loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom loss function since there are no ground truths label\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 160)\n"
     ]
    }
   ],
   "source": [
    "facenet = load_facenet_model(\"facenet/facenet_weights.h5\")\n",
    "print(facenet.input_shape[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_siamese_model(facenet)\n",
    "\n",
    "model.compile(loss=identity_loss, optimizer=optimizers.Adam(learning_rate=1e-4))\n",
    "\n",
    "callbacks=[EarlyStopping(\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    "    monitor=\"val_loss\",\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(3, 160, 160, 3) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m     train_dataset,\n\u001b[0;32m      3\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[0;32m      4\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[0;32m      6\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\TJTHAN~1\\AppData\\Local\\Temp\\__autograph_generated_file7cecwlvw.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\tjthanapat\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(3, 160, 160, 3) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50, \n",
    "    batch_size=64,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
